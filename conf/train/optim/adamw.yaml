# clip the gradient to have l2 norm at most this value
gradient_clip_val: 10.0

# lr schedule shape and scale
lr_warmup: 0.1
lr_decay: linear
lr: 0.0003
grad_schedule: False

beta1: 0.9
beta2: 0.999

use_max: False

weight_decay: 0.01
batch_size: 4 # number of examples placed on each GPU

accumulation_steps: 1

# options for exponential balancer
do_exp_balancing: False
exp_balancing:
  beta1: 1.0
  beta2: 1.0
  s_init: 1e-8
  exp_scaling: 1.0
  granularity: 'global'
  exp_type: 'standard'

# options for mechanic
mechanize: False
mechanic:
  weight_decay: 0.01
  optimistic: False
  incremental: False
  randomize_incremental: False
  per_layer: False
  use_one_beta: False
  single_beta_val: 1.0
  max_tuner_output: null
  use_incremental_variation: False
  averaging_momentum: 0.0
  freeze_s_iteration: null
  randomize_after_freeze: False
  tuner_decay_schedule: constant
  tuner_lr: 1.0
  optax:
    betas2: null
    betas: [0.9, 0.99, 0.999, 0.9999, 0.99999, 0.999999]
    num_iter: 50000
    bet_fraction_type: sqrt

random_scale_type: none
base_opt: capped_cb
update_type: diag_sum
max_bet: 0.1
cb_eps: 1e-6
cb_var_eps: 0.0
cb_reward_beta: 1.0
cb_use_dim: False
cb_grad_norm_type: l2
cb_clip_bet_fraction: False
cb_s_decay: False
cb_do_project_reduction: False
cb_do_log_wealth_grad: False
grad_stats_beta: 0.999
randomize_schedule: False
log_inner_product: True
cap: 1e-2

# whether to apply the schedule before mechanic (True) or after mechanic (False)
bake_schedule: True

name: "adamw"

momentum: 0.9


