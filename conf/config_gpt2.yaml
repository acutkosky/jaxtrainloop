model:
  name: gpt

  dim: 768
  num_heads: 12
  bias: False
  num_blocks: 12
  context_length: 1024
  rescale_residuals: False 

train:
  total_duration: "1min"
  valid_frequency: "30it"
  valid_duration: "10it"

  dataset: "c4"
  data_path: "/projectnb/aclab/datasets/c4/en/"

  valid_key: "validation"
  test_key: "test"
  
  max_steps: 10000

  # whether to use automatic mixed precision
  use_amp: True
  # value to cast to in mixed precision training.
  precision: float16

  # clip the gradient to have l2 norm at most this value
  gradient_clip_val: 10.0

  # lr schedule shape and scale
  lr_warmup: 1000
  lr_decay: linear
  lr: 0.0001
  
  weight_decay: 0.01
  batch_size: 4 # number of examples placed on each GPU

  # options for mechanic
  mechanize: False
  mech_lambda: 0.01
  # whether to apply the schedule before mechanic (True) or after mechanic (False)
  bake_schedule: True

  optimizer: "adamw"

  wandb_project: null
  wandb_logs_per_sec: 1.0
  
  # this will slow down computation a bit (I believe due to extra GPU/CPU communication),
  # but will log more stuff (like learning rates).
  # Still working on nice way to do this logging - we really should only incur one communication
  # round per iteration and I don't think the logging data should significantly impact it.
  log_callback_data: True


  dataloader_workers: 2
    



